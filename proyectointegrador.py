# -*- coding: utf-8 -*-
"""ProyectoIntegrador.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sbMcnh7nh_6v3pgdvQG0Yg0J0W89eGAc

# **Clasificación de Lesiones en Corredores**

### **Exploratory Data Analysis (EDA)**
"""

# Importamos las librerías necesarias
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler

# 1. Cargar el dataset

df = pd.read_csv('/content/sample_data/run_data_meta.csv', encoding='utf-8')
print("Dimensiones del dataset:", df.shape)

# 2. Eliminar columnas identificadoras que no aporten valor predictivo
df = df.drop(columns=["sub_id", "filename"])

# 3. Exploración inicial
print("\nPrimeras 5 filas:")
print(df.head())

print("\nInformación del dataset:")
print(df.info())

print("\nEstadísticas descriptivas de variables numéricas:")
print(df.describe())

"""### **Limpieza y Preprocesamiento**"""

# 4. Chequeo de valores faltantes
print("\nCantidad de valores faltantes por columna:")
print(df.isnull().sum())

# 5. Procesar la columna de fecha
df["datestring"] = pd.to_datetime(df["datestring"], errors="coerce")
# Extraer características de fecha (opcional)
df["Year"] = df["datestring"].dt.year
df["Month"] = df["datestring"].dt.month
df["Day"] = df["datestring"].dt.day

df = df.drop(columns=["datestring"])

# 6. Procesar columnas que representan tiempos: RaceTimeHrs, RaceTimeMins, RaceTimeSecs
# Convertirlas a numérico (en caso de que estén como texto)
df["RaceTimeHrs"] = pd.to_numeric(df["RaceTimeHrs"], errors="coerce")
df["RaceTimeMins"] = pd.to_numeric(df["RaceTimeMins"], errors="coerce")
df["RaceTimeSecs"] = pd.to_numeric(df["RaceTimeSecs"], errors="coerce")

# 7. Identificar columnas numéricas y categóricas
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = df.select_dtypes(include=["object"]).columns.tolist()

print("Columnas numéricas a normalizar:", numeric_cols)
print("Columnas categóricas a codificar:", categorical_cols)

# 7. Aplicar One-Hot Encoding a las variables categóricas
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# 8. Normalizar las columnas numéricas (tanto las originales como las generadas por features de fecha, por ejemplo)
scaler = MinMaxScaler(feature_range=(0, 1))

df_encoded[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])

# Resultado final
print("Dataset final con todas las columnas en rango [0, 1]:")
print(df_encoded.head())

# 9. Eliminación de filas duplicadas (si las hubiera)
duplicados = df.duplicated().sum()
print("\nNúmero de filas duplicadas:", duplicados)
if duplicados > 0:
    df = df.drop_duplicates()
    print("Se eliminaron los duplicados. Nuevo shape:", df.shape)

# Reemplazamos 'Unknown' por NaN en todo el dataframe
df.replace("Unknown", np.nan, inplace=True)

# 11. Análisis de variables categóricas
# Se muestran el número de valores únicos y una muestra de la distribución.
categoricas = df.select_dtypes(include=['object']).columns
print("\nVariables categóricas encontradas:", list(categoricas))
for col in categoricas:
    print(f"\nColumna: {col}")
    print("Número de valores únicos:", df[col].nunique())
    print(df[col].value_counts().head(10))  # muestra los 10 valores más frecuentes

"""## **Visualización Gráfica**

### **Histograma**
"""

# Visualización: Histograma de las variables numéricas
df.hist(figsize=(10, 10))
plt.suptitle("Histograma de variables numéricas", fontsize=16)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

# 12. Visualización de distribuciones para variables numéricas


# Seleccionar todas las columnas numéricas
numericas = df_encoded.select_dtypes(include=[np.number]).columns.tolist()
print("\nVariables numéricas encontradas:", numericas)

for col in numericas:
    plt.figure(figsize=(8, 4))
    # Extraer valores únicos (sin considerar NaN)
    valores_unicos = df_encoded[col].dropna().unique()

    # Si la variable es binaria (por ejemplo, solo tiene 0 y 1) usamos un gráfico de barras
    if len(valores_unicos) == 2:
        # Obtenemos el conteo de cada valor
        conteo = df_encoded[col].value_counts().sort_index()
        conteo.plot(kind='bar', edgecolor='k')
        plt.title(f"Distribución de '{col}' (binaria)")
        plt.xlabel(col)
        plt.ylabel("Frecuencia")
    else:
        # Para variables continuas, usamos un histograma
        plt.hist(df_encoded[col].dropna(), bins=30, edgecolor='k')
        plt.title(f"Distribución de '{col}'")
        plt.xlabel(col)
        plt.ylabel("Frecuencia")

    plt.show()

"""### **Matriz de Correlación de Pearson**"""

import matplotlib.pyplot as plt
import seaborn as sns

# Asegurarse de tener la lista de columnas numéricas
numeric_cols = df.select_dtypes(include=['number']).columns.tolist()

# Calcular y visualizar la matriz de correlación
plt.figure(figsize=(10, 8))
corr = df[numeric_cols].corr()
sns.heatmap(corr, annot=True, cmap='coolwarm')
plt.title('Matriz de correlación de Pearson')
plt.show()

# 10. Detección y tratamiento de outliers en variables numéricas
# Se utiliza la regla del IQR para limitar los valores extremos.
for col in numericas:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])
    df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])
    print(f"Se han acotado los outliers de la columna '{col}' con límites [{lower_bound}, {upper_bound}].")

# 12. Visualización final y revisión del dataset preprocesado
print("\nDataset preprocesado:")
print(df.head())

import pandas as pd
import re


# Función para limpiar la columna InjDefn
def limpiar_injdefn(texto):
    # Si el valor es nulo, se devuelve directamente
    if pd.isnull(texto):
        return texto
    # Convertir a string, por seguridad
    texto = str(texto)
    # Eliminar espacios al inicio y al final
    texto = texto.strip()
    # Convertir a minúsculas para unificar la forma
    texto = texto.lower()
    # Eliminar caracteres especiales (dejando solo letras, números y espacios)
    texto = re.sub(r'[^\w\s]', '', texto)

    return texto

# Aplicamos la función de limpieza a la columna InjDefn
if 'InjDefn' in df.columns:
    df['InjDefn'] = df['InjDefn'].apply(limpiar_injdefn)
    print("Limpieza de la columna 'InjDefn' completada.")
    print("Valores únicos después de la limpieza:")
    print(df['InjDefn'].unique())
else:
    print("La columna 'InjDefn' no se encuentra en el dataset.")

"""## **Binarización de la Variable "InjDefn"**"""

# Asegurarnos de que la columna 'InjDefn' existe en el DataFrame
if 'InjDefn' in df.columns:
    # Convertir la columna a string, pasarla a minúsculas y quitar espacios extra
    df['InjDefn'] = df['InjDefn'].astype(str).str.lower().str.strip()

    # Calcular la distribución de la columna antes de binarización
    distribution = df['InjDefn'].value_counts()
    print("Distribución de la clase 'InjDefn' (antes de la binarización):")
    print(distribution)

    # Graficar la distribución usando un gráfico de barras
    plt.figure(figsize=(10, 6))
    distribution.plot(kind='bar', edgecolor='black', color='skyblue')
    plt.xlabel("Categorías en 'InjDefn'")
    plt.ylabel("Frecuencia")
    plt.title("Distribución de la clase 'InjDefn' (antes de la binarización)")
    plt.xticks(rotation=45)
    plt.show()
else:
    print("La columna 'InjDefn' no se encuentra en el DataFrame.")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Verificar que la columna 'InjDefn' existe
if 'InjDefn' in df.columns:
    # Convertir la columna a string, poner en minúsculas, quitar espacios
    df['InjDefn'] = df['InjDefn'].astype(str).str.lower().str.strip()

    # Reemplazar el string 'nan' (resultado de convertir np.nan a string) por un valor nulo real
    df['InjDefn'] = df['InjDefn'].replace('nan', np.nan)

    # Imputar los valores faltantes: aquí los tratamos como "no lesión"
    df['InjDefn'] = df['InjDefn'].fillna('none')

    # Definir qué valores se considerarán como "sin lesión"
    no_lesion = ['none', 'no lesion', 'no injury', '']  # Ajusta según lo que refleje tu dataset

    # Binarización: 0 si no hay lesión, 1 si hay lesión
    df['InjDefn_binary'] = df['InjDefn'].apply(lambda x: 0 if x in no_lesion else 1)

    # Imprimir la distribución resultante
    distribution = df['InjDefn_binary'].value_counts().sort_index()
    print("Distribución de InjDefn binarizada:")
    print(distribution)

    # Graficar el histograma de la variable binarizada
    plt.figure(figsize=(6,4))
    plt.bar(distribution.index.astype(str), distribution.values, color='skyblue', edgecolor='black')
    plt.xlabel("InjDefn_binary (0: Sin lesión, 1: Con lesión)")
    plt.ylabel("Frecuencia")
    plt.title("Histograma de la variable InjDefn (binarizada)")
    plt.show()
else:
    print("La columna 'InjDefn' no se encuentra en el DataFrame.")

"""### **Boxplots**"""

# Convertir InjDefn a valores numéricos para el boxplot
df['InjDefn_numeric'] = df['InjDefn'].astype('category').cat.codes

plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
sns.boxplot(y=df['InjDefn_numeric'])
plt.title('Boxplot de InjDefn')

plt.subplot(1, 2, 2)
sns.boxplot(y=df['InjDefn_binary'])
plt.title('Boxplot de InjDefn_binary')

plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Verificar que la columna 'InjDefn' existe en el DataFrame
if 'InjDefn' in df.columns:
    # Limpieza básica: convertir a string, poner en minúsculas y quitar espacios extremos
    df['InjDefn_clean'] = df['InjDefn'].astype(str).str.lower().str.strip()

    # Mostrar los valores únicos para tener una idea de la diversidad de categorías
    print("Valores únicos de 'InjDefn' (limpios):")
    print(df['InjDefn_clean'].unique())

    # Convertir la variable categórica a una variable numérica mediante códigos arbitrarios
    df['InjDefn_codes'] = df['InjDefn_clean'].astype('category').cat.codes


    # Generar el boxplot de la variable codificada
    plt.figure(figsize=(6, 4))
    plt.boxplot(df['InjDefn_codes'].dropna(), patch_artist=True,
                boxprops=dict(facecolor='lightblue', color='blue'),
                medianprops=dict(color='red'))
    plt.xlabel("InjDefn (encoded)")
    plt.title("Boxplot de InjDefn antes de la binarización")
    plt.show()
else:
    print("La columna 'InjDefn' no se encuentra en el DataFrame.")

"""### **Visualización t-SNE**"""

from sklearn.manifold import TSNE
from sklearn.impute import SimpleImputer

# 3. Selección de columnas numéricas para el análisis (excluyendo 'InjDefn_binary')
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
features = df[numeric_cols].drop(columns=['InjDefn_binary'], errors='ignore')

# 4. Revisar la cantidad de valores nulos en cada columna
print("Valores nulos en cada columna:")
print(features.isnull().sum())

# 5. Imputar los valores faltantes utilizando la media de cada columna
imputer = SimpleImputer(strategy='mean')
features_imputed = imputer.fit_transform(features)

# 6. Aplicar TSNE a los datos imputados
tsne = TSNE(n_components=2, random_state=42)
tsne_results = tsne.fit_transform(features_imputed)

# Agregar los resultados TSNE al DataFrame
df['TSNE-1'] = tsne_results[:, 0]
df['TSNE-2'] = tsne_results[:, 1]

# 7. Visualizar TSNE: scatter plot coloreado según InjDefn_binary
plt.figure(figsize=(8, 6))
sns.scatterplot(x='TSNE-1', y='TSNE-2', hue='InjDefn_binary', data=df, palette='viridis')
plt.title('Visualización TSNE')
plt.xlabel('TSNE-1')
plt.ylabel('TSNE-2')
plt.legend(title='InjDefn_binary', loc='best')
plt.show()

"""## **Modelos de Shallow Learning**

### **Random Forest**

#### **Random Forest con 75**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (classification_report, accuracy_score, roc_auc_score,
                             confusion_matrix, precision_score, recall_score, f1_score)


# 1. Seleccionar las variables predictoras eliminando 'InjDefn' e 'InjDefn_binary'
features = df.drop(columns=['InjDefn', 'InjDefn_binary'], errors='ignore')

# Convertir variables categóricas a numéricas con one-hot encoding
features = pd.get_dummies(features, drop_first=True)

# 2. Definir la variable objetivo
target = df['InjDefn_binary']

# 3. Dividir el dataset en entrenamiento (70%) y prueba (30%)
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)

# 4. Configuración para 75 estimadores
n_estimators = 75

# 5. Validación cruzada estratificada de 10 folds sobre el conjunto de entrenamiento
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Listas para almacenar las métricas de cada fold
fold_accuracies = []
fold_auc_scores = []
fold_precisions = []
fold_recalls = []
fold_f1s = []

print("Evaluación de métricas por cada fold (75 Estimadores)")
print("-" * 50)

# Iterar sobre cada fold
for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train), 1):
    # Dividir los datos según el fold actual
    X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]
    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]

    # Crear y entrenar el modelo para este fold
    rf_fold = RandomForestClassifier(n_estimators=n_estimators, random_state=42)
    rf_fold.fit(X_train_fold, y_train_fold)

    # Realizar predicciones en el fold de validación
    y_val_pred = rf_fold.predict(X_val_fold)
    y_val_pred_proba = rf_fold.predict_proba(X_val_fold)[:, 1]

    # Calcular las métricas para el fold actual
    acc = accuracy_score(y_val_fold, y_val_pred)
    auc = roc_auc_score(y_val_fold, y_val_pred_proba)
    precision = precision_score(y_val_fold, y_val_pred)
    recall = recall_score(y_val_fold, y_val_pred)
    f1 = f1_score(y_val_fold, y_val_pred)

    # Almacenar los resultados
    fold_accuracies.append(acc)
    fold_auc_scores.append(auc)
    fold_precisions.append(precision)
    fold_recalls.append(recall)
    fold_f1s.append(f1)

    print(f"Fold {fold}")
    print(classification_report(y_val_fold, y_val_pred))
    print(f"Exactitud: {acc:.4f}")
    print(f"AUC: {auc:.4f}")
    print(f"Precisión: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1: {f1:.4f}")
    print("-" * 50)

# Calcular los promedios y las desviaciones estándar de las métricas
avg_accuracy = np.mean(fold_accuracies)
avg_auc = np.mean(fold_auc_scores)
avg_precision = np.mean(fold_precisions)
avg_recall = np.mean(fold_recalls)
avg_f1 = np.mean(fold_f1s)

std_accuracy  = np.std(fold_accuracies,  ddof=1)
std_auc       = np.std(fold_auc_scores,   ddof=1)
std_precision = np.std(fold_precisions,    ddof=1)
std_recall    = np.std(fold_recalls,       ddof=1)
std_f1        = np.std(fold_f1s,           ddof=1)

print("Resumen de CV (10 folds) - 75 Estimadores:")
print("Exactitud promedio: {:.4f} ± {:.4f}".format(avg_accuracy, std_accuracy))
print("AUC promedio: {:.4f} ± {:.4f}".format(avg_auc, std_auc))

# Crear tabla resumen con todas las métricas promediadas y sus desviaciones estándar
tabla_resumen = pd.DataFrame({
    "Métrica": ["Score Promedio", "AUC Promedio", "Accuracy Promedio",
                "Precisión Promedio", "Recall Promedio", "F1 Promedio"],
    "Promedio": [avg_accuracy, avg_auc, avg_accuracy, avg_precision, avg_recall, avg_f1],
    "±": [std_accuracy, std_auc, std_accuracy, std_precision, std_recall, std_f1]
})

print("\nTabla resumen de métricas promedio (10-Fold CV):")
print(tabla_resumen)

# 6. Entrenar el modelo final con 75 estimadores usando todo el conjunto de entrenamiento
rf_final = RandomForestClassifier(n_estimators=n_estimators, random_state=42)
rf_final.fit(X_train, y_train)
y_test_pred = rf_final.predict(X_test)
y_test_pred_proba = rf_final.predict_proba(X_test)[:, 1]

print("\nReporte de clasificación en el conjunto de prueba:")
print(classification_report(y_test, y_test_pred))
print("Exactitud en el conjunto de prueba: {:.4f}".format(accuracy_score(y_test, y_test_pred)))
print("AUC en el conjunto de prueba: {:.4f}".format(roc_auc_score(y_test, y_test_pred_proba)))

# 7. Calcular y visualizar la matriz de confusión en el conjunto de prueba
cm = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicción")
plt.ylabel("Valor Real")
plt.title("Matriz de Confusión para Random Forest con 75 Estimadores")
plt.show()

"""#### **Random Forest con 100**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (classification_report, accuracy_score, roc_auc_score,
                             confusion_matrix, precision_score, recall_score, f1_score)


# 1. Seleccionar las variables predictoras eliminando 'InjDefn' e 'InjDefn_binary'
features = df.drop(columns=['InjDefn', 'InjDefn_binary'], errors='ignore')

# Convertir variables categóricas a numéricas con one-hot encoding
features = pd.get_dummies(features, drop_first=True)

# 2. Definir la variable objetivo
target = df['InjDefn_binary']

# 3. Dividir el dataset en entrenamiento (70%) y prueba (30%)
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)

# 4. Configuración para 100 estimadores
n_estimators = 100

# 5. Validación cruzada estratificada de 10 folds
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Listas para almacenar las métricas por fold
fold_accuracies = []
fold_auc_scores = []
fold_precisions = []
fold_recalls = []
fold_f1s = []

print("Random Forest con 100 estimadores")
print("-" * 50)

# Iterar sobre los folds
for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train), 1):
    X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]
    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]

    rf_fold = RandomForestClassifier(n_estimators=n_estimators, random_state=42)
    rf_fold.fit(X_train_fold, y_train_fold)

    y_val_pred = rf_fold.predict(X_val_fold)
    y_val_pred_proba = rf_fold.predict_proba(X_val_fold)[:, 1]

    # Calcular métricas para el fold actual
    acc = accuracy_score(y_val_fold, y_val_pred)
    auc = roc_auc_score(y_val_fold, y_val_pred_proba)
    precision = precision_score(y_val_fold, y_val_pred)
    recall = recall_score(y_val_fold, y_val_pred)
    f1 = f1_score(y_val_fold, y_val_pred)

    # Guardar los resultados
    fold_accuracies.append(acc)
    fold_auc_scores.append(auc)
    fold_precisions.append(precision)
    fold_recalls.append(recall)
    fold_f1s.append(f1)

    print(f"Fold {fold}")
    print(classification_report(y_val_fold, y_val_pred))
    print(f"Exactitud: {acc:.4f}")
    print(f"AUC: {auc:.4f}")
    print(f"Precisión: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1: {f1:.4f}")
    print("-" * 50)

# Calcular promedios de las métricas
avg_accuracy = np.mean(fold_accuracies)
avg_auc = np.mean(fold_auc_scores)
avg_precision = np.mean(fold_precisions)
avg_recall = np.mean(fold_recalls)
avg_f1 = np.mean(fold_f1s)

# Calcular las desviaciones estándar de las métricas
std_accuracy  = np.std(fold_accuracies,  ddof=1)
std_auc       = np.std(fold_auc_scores,   ddof=1)
std_precision = np.std(fold_precisions,    ddof=1)
std_recall    = np.std(fold_recalls,       ddof=1)
std_f1        = np.std(fold_f1s,           ddof=1)

print("Resumen de CV (10 folds) - 100 Estimadores:")
print("Exactitud promedio: {:.4f} ± {:.4f}".format(avg_accuracy, std_accuracy))
print("AUC promedio: {:.4f} ± {:.4f}".format(avg_auc, std_auc))

# Crear tabla resumen con todas las métricas promediadas y sus desviaciones
tabla_resumen = pd.DataFrame({
    "Métrica": ["Score Promedio", "AUC Promedio", "Accuracy Promedio",
                "Precisión Promedio", "Recall Promedio", "F1 Promedio"],
    "Promedio": [avg_accuracy, avg_auc, avg_accuracy, avg_precision, avg_recall, avg_f1],
    "±": [std_accuracy, std_auc, std_accuracy, std_precision, std_recall, std_f1]
})

print("\nTabla resumen de métricas promedio (10-Fold CV):")
print(tabla_resumen)

# 6. Entrenar el modelo final con 100 estimadores usando todo el conjunto de entrenamiento
rf_final = RandomForestClassifier(n_estimators=n_estimators, random_state=42)
rf_final.fit(X_train, y_train)
y_test_pred = rf_final.predict(X_test)
y_test_pred_proba = rf_final.predict_proba(X_test)[:, 1]

print("\nReporte en el conjunto de prueba (100 Estimadores):")
print(classification_report(y_test, y_test_pred))
print("Exactitud en el conjunto de prueba: {:.4f}".format(accuracy_score(y_test, y_test_pred)))
print("AUC en el conjunto de prueba: {:.4f}".format(roc_auc_score(y_test, y_test_pred_proba)))

# 7. Visualizar la matriz de confusión
cm = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicción")
plt.ylabel("Real")
plt.title("Matriz de Confusión (100 Estimadores)")
plt.show()

"""#### **Random Forest con 125**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (classification_report, accuracy_score, roc_auc_score,
                             confusion_matrix, precision_score, recall_score, f1_score)


# 1. Seleccionar las variables predictoras eliminando 'InjDefn' e 'InjDefn_binary'
features = df.drop(columns=['InjDefn', 'InjDefn_binary'], errors='ignore')

# Convertir variables categóricas a numéricas con one-hot encoding
features = pd.get_dummies(features, drop_first=True)

# 2. Definir la variable objetivo
target = df['InjDefn_binary']

# 3. Dividir el dataset en entrenamiento (70%) y prueba (30%)
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)

# 4. Configuración para 125 estimadores
n_estimators = 125

# 5. Validación cruzada estratificada de 10 folds
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Listas para almacenar las métricas de cada fold
fold_accuracies = []
fold_auc_scores = []
fold_precisions = []
fold_recalls = []
fold_f1s = []

print("Random Forest con 125 estimadores")
print("-" * 50)

# Iterar sobre los folds
for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train), 1):
    X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]
    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]

    rf_fold = RandomForestClassifier(n_estimators=n_estimators, random_state=42)
    rf_fold.fit(X_train_fold, y_train_fold)

    y_val_pred = rf_fold.predict(X_val_fold)
    y_val_pred_proba = rf_fold.predict_proba(X_val_fold)[:, 1]

    # Calcular las métricas para el fold actual
    acc = accuracy_score(y_val_fold, y_val_pred)
    auc = roc_auc_score(y_val_fold, y_val_pred_proba)
    precision = precision_score(y_val_fold, y_val_pred)
    recall = recall_score(y_val_fold, y_val_pred)
    f1 = f1_score(y_val_fold, y_val_pred)

    # Almacenar resultados
    fold_accuracies.append(acc)
    fold_auc_scores.append(auc)
    fold_precisions.append(precision)
    fold_recalls.append(recall)
    fold_f1s.append(f1)

    print(f"Fold {fold}")
    print(classification_report(y_val_fold, y_val_pred))
    print(f"Exactitud: {acc:.4f}")
    print(f"AUC: {auc:.4f}")
    print(f"Precisión: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1: {f1:.4f}")
    print("-" * 50)

# Calcular promedios y desviaciones estándar
avg_accuracy = np.mean(fold_accuracies)
avg_auc = np.mean(fold_auc_scores)
avg_precision = np.mean(fold_precisions)
avg_recall = np.mean(fold_recalls)
avg_f1 = np.mean(fold_f1s)

std_accuracy  = np.std(fold_accuracies,  ddof=1)
std_auc       = np.std(fold_auc_scores,   ddof=1)
std_precision = np.std(fold_precisions,    ddof=1)
std_recall    = np.std(fold_recalls,       ddof=1)
std_f1        = np.std(fold_f1s,           ddof=1)


print("Resumen de CV (10 folds) - 125 Estimadores:")
print("Exactitud promedio: {:.4f} ± {:.4f}".format(avg_accuracy, std_accuracy))
print("AUC promedio: {:.4f} ± {:.4f}".format(avg_auc, std_auc))

# Crear tabla resumen con todas las métricas
tabla_resumen = pd.DataFrame({
    "Métrica": ["Score Promedio", "AUC Promedio", "Accuracy Promedio",
                "Precisión Promedio", "Recall Promedio", "F1 Promedio"],
    "Promedio": [avg_accuracy, avg_auc, avg_accuracy, avg_precision, avg_recall, avg_f1],
    "±": [std_accuracy, std_auc, std_accuracy, std_precision, std_recall, std_f1]
})

print("\nTabla resumen de métricas promedio (10-Fold CV):")
print(tabla_resumen)

# 6. Entrenar el modelo final con 125 estimadores usando todo el conjunto de entrenamiento
rf_final = RandomForestClassifier(n_estimators=n_estimators, random_state=42)
rf_final.fit(X_train, y_train)
y_test_pred = rf_final.predict(X_test)
y_test_pred_proba = rf_final.predict_proba(X_test)[:, 1]

print("\nReporte en el conjunto de prueba (125 Estimadores):")
print(classification_report(y_test, y_test_pred))
print("Exactitud en el conjunto de prueba: {:.4f}".format(accuracy_score(y_test, y_test_pred)))
print("AUC en el conjunto de prueba: {:.4f}".format(roc_auc_score(y_test, y_test_pred_proba)))

# 7. Visualizar la matriz de confusión
cm = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicción")
plt.ylabel("Real")
plt.title("Matriz de Confusión (125 Estimadores)")
plt.show()

"""#### **MLP**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
import seaborn as sns

# Seleccionar las variables predictoras (se eliminan 'InjDefn' y 'InjDefn_binary')
features = df.drop(columns=['InjDefn', 'InjDefn_binary'], errors='ignore')
# Convertir variables categóricas a numéricas (one-hot encoding)
features = pd.get_dummies(features, drop_first=True)

# Definir la variable objetivo (etiquetas 0/1, sin one-hot)
target = df['InjDefn_binary']

# Dividir en entrenamiento (70%) y prueba (30%)
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)

# Imputar valores faltantes (NaN) usando la media
imputer = SimpleImputer(strategy='mean')
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)

# Escalar los datos
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Determinar la dimensión de entrada
input_dim = X_train.shape[1]
print("Dimensión de entrada:", input_dim)

"""#### **MLP 1**"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical

from sklearn.model_selection import KFold
from sklearn.metrics import (
    confusion_matrix,
    accuracy_score,
    roc_auc_score,
    precision_score,
    recall_score,
    f1_score
)


if not isinstance(X_train, np.ndarray):
    X_train = X_train.values
if not isinstance(y_train, np.ndarray):
    y_train = y_train.values
if not isinstance(X_test, np.ndarray):
    X_test = X_test.values
if not isinstance(y_test, np.ndarray):
    y_test = y_test.values

# Para usar categorical crossentropy con softmax en un problema binario:
y_train_cat = to_categorical(y_train, num_classes=2)
y_test_cat  = to_categorical(y_test,  num_classes=2)

# Definir input_dim a partir del número de características
input_dim = X_train.shape[1]

def create_model(input_dim, num_classes=2):
    model = Sequential()
    model.add(Dense(2 * input_dim, input_dim=input_dim, activation='relu'))
    model.add(Dense(input_dim, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(
        optimizer=Adam(learning_rate=1e-4),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=20,
    restore_best_weights=True
)

# --- K-Fold Cross Validation (10 folds) ---
kf = KFold(n_splits=10, shuffle=True, random_state=42)
fold_results = []
fold_no = 1

for train_index, val_index in kf.split(X_train):
    X_fold_train = X_train[train_index]
    X_fold_val   = X_train[val_index]
    y_fold_train = y_train_cat[train_index]
    y_fold_val   = y_train_cat[val_index]

    model = create_model(input_dim)
    history = model.fit(
        X_fold_train, y_fold_train,
        validation_data=(X_fold_val, y_fold_val),
        epochs=1000,
        batch_size=32,
        callbacks=[early_stopping],
        verbose=0
    )

    # Evaluación Keras
    loss, keras_acc = model.evaluate(X_fold_val, y_fold_val, verbose=0)

    # Predicciones
    y_val_prob = model.predict(X_fold_val, verbose=0)
    y_val_pred = np.argmax(y_val_prob, axis=1)
    y_val_true = np.argmax(y_fold_val, axis=1)

    # Métricas sklearn
    acc   = accuracy_score(y_val_true, y_val_pred)
    auc   = roc_auc_score(y_val_true, y_val_prob[:,1])
    prec  = precision_score(y_val_true, y_val_pred)
    rec   = recall_score(y_val_true, y_val_pred)
    f1    = f1_score(y_val_true, y_val_pred)

    fold_results.append({
        "Fold": fold_no,
        "Epochs": len(history.history['loss']),
        "Val_Loss": loss,
        "Keras_Acc": keras_acc,
        "Accuracy": acc,
        "AUC": auc,
        "Precision": prec,
        "Recall": rec,
        "F1": f1
    })

    print(f"Fold {fold_no} → Loss: {loss:.4f}, Acc: {acc:.4f}, "
          f"AUC: {auc:.4f}, Prec: {prec:.4f}, Rec: {rec:.4f}, F1: {f1:.4f}")
    fold_no += 1

# Resultados por fold
results_df = pd.DataFrame(fold_results)
print("\nResultados por fold:")
print(results_df.to_string(index=False))

# -----------------------------
# Cálculo de métricas promedio y desviación estándar
stats = results_df[[
    "Val_Loss", "Keras_Acc",
    "Accuracy", "AUC",
    "Precision", "Recall", "F1"
]]
summary = stats.agg(["mean", "std"]).transpose().reset_index()
summary.columns = ["Métrica", "Promedio", "Desviación estándar"]

print("\nMétricas promedio y SD (10 folds):")
print(summary.to_markdown(index=False, floatfmt=".4f"))

# -----------------------------
# Entrenamiento final con todo el set de entrenamiento
final_model = create_model(input_dim)
history_final = final_model.fit(
    X_train, y_train_cat,
    validation_split=0.2,
    epochs=1000,
    batch_size=32,
    callbacks=[early_stopping],
    verbose=1
)

# Evaluación en test
loss_test, acc_test = final_model.evaluate(X_test, y_test_cat, verbose=0)
print(f"\nModelo final - Loss test: {loss_test:.4f}, Accuracy test: {acc_test:.4f}")

# Matriz de confusión
y_pred_prob = final_model.predict(X_test, verbose=0)
y_pred = np.argmax(y_pred_prob, axis=1)
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicción')
plt.ylabel('Valor Real')
plt.title('Matriz de Confusión - Modelo Final')
plt.show()

# Curva de Loss
plt.figure(figsize=(8, 5))
plt.plot(history_final.history['loss'], label='Train Loss')
plt.plot(history_final.history['val_loss'], label='Validation Loss')
plt.xlabel('Épocas')
plt.ylabel('Loss')
plt.title('Curva de Loss durante el Entrenamiento')
plt.legend()
plt.show()

"""#### **MLP 2**"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical  # Para convertir etiquetas a one-hot

from sklearn.model_selection import KFold
from sklearn.metrics import (
    confusion_matrix,
    classification_report,
    accuracy_score,
    roc_auc_score,
    precision_score,
    recall_score,
    f1_score
)

if not isinstance(X_train, np.ndarray):
    X_train = X_train.values
if not isinstance(y_train, np.ndarray):
    y_train = y_train.values
if not isinstance(X_test, np.ndarray):
    X_test = X_test.values
if not isinstance(y_test, np.ndarray):
    y_test = y_test.values

# One‑hot para categorical_crossentropy (problema binario)
y_train_cat = to_categorical(y_train, num_classes=2)
y_test_cat  = to_categorical(y_test,  num_classes=2)

# Input dimension
input_dim = X_train.shape[1]
# Tamaño primera capa oculta = 50% de input_dim
hidden1_mlp2 = max(1, int(0.5 * input_dim))

def create_mlp2_model(input_dim):
    model = Sequential()
    model.add(Dense(hidden1_mlp2, input_dim=input_dim, activation='relu'))
    model.add(Dense(input_dim, activation='relu'))
    model.add(Dense(2, activation='softmax'))
    model.compile(
        optimizer=Adam(learning_rate=1e-4),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=20,
    restore_best_weights=True
)

# --- K-Fold Cross Validation (10 folds) ---
kf = KFold(n_splits=10, shuffle=True, random_state=42)
fold_results = []
fold_no = 1

for train_index, val_index in kf.split(X_train):
    X_fold_train, X_fold_val = X_train[train_index], X_train[val_index]
    y_fold_train, y_fold_val = y_train_cat[train_index], y_train_cat[val_index]

    model = create_mlp2_model(input_dim)
    history = model.fit(
        X_fold_train, y_fold_train,
        validation_data=(X_fold_val, y_fold_val),
        epochs=1000,
        batch_size=32,
        callbacks=[early_stopping],
        verbose=0
    )

    # Evaluación Keras
    loss, keras_acc = model.evaluate(X_fold_val, y_fold_val, verbose=0)

    # Predicciones para métricas adicionales
    y_val_prob = model.predict(X_fold_val, verbose=0)
    y_val_pred = np.argmax(y_val_prob, axis=1)
    y_val_true = np.argmax(y_fold_val, axis=1)

    # Métricas sklearn
    acc   = accuracy_score(y_val_true, y_val_pred)
    auc   = roc_auc_score(y_val_true, y_val_prob[:,1])
    prec  = precision_score(y_val_true, y_val_pred)
    rec   = recall_score(y_val_true, y_val_pred)
    f1    = f1_score(y_val_true, y_val_pred)

    fold_results.append({
        "Fold": fold_no,
        "Epochs": len(history.history['loss']),
        "Val_Loss": loss,
        "Keras_Acc": keras_acc,
        "Accuracy": acc,
        "AUC": auc,
        "Precision": prec,
        "Recall": rec,
        "F1": f1
    })

    print(f"Fold {fold_no} → Loss: {loss:.4f}, Acc: {acc:.4f}, "
          f"AUC: {auc:.4f}, Prec: {prec:.4f}, Rec: {rec:.4f}, F1: {f1:.4f}")
    fold_no += 1

# DataFrame de resultados por fold
results_df = pd.DataFrame(fold_results)
print("\nResultados por fold:")
print(results_df.to_string(index=False))

# -----------------------------
# Tabla resumen: promedio y desviación estándar
stats = results_df[[
    "Val_Loss", "Keras_Acc",
    "Accuracy", "AUC",
    "Precision", "Recall", "F1"
]]
summary = stats.agg(["mean", "std"]).transpose().reset_index()
summary.columns = ["Métrica", "Promedio", "Desviación estándar"]

print("\nMétricas promedio y SD (10 folds):")
print(summary.to_markdown(index=False, floatfmt=".4f"))

# -------------------------------------------------------------------
# Entrenamiento final con todo el set de entrenamiento
final_model = create_mlp2_model(input_dim)
history_final = final_model.fit(
    X_train, y_train_cat,
    validation_split=0.2,
    epochs=1000,
    batch_size=32,
    callbacks=[early_stopping],
    verbose=1
)

# Evaluación en test
loss_test, acc_test = final_model.evaluate(X_test, y_test_cat, verbose=0)
print(f"\nMLP 2 - Loss test: {loss_test:.4f}, Accuracy test: {acc_test:.4f}")

# Reporte y matriz de confusión en test
y_pred_prob = final_model.predict(X_test, verbose=0)
y_pred = np.argmax(y_pred_prob, axis=1)
y_true = np.argmax(y_test_cat, axis=1)

print("\nReporte de Clasificación en test:")
print(classification_report(y_true, y_pred))

cm2 = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm2, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicción')
plt.ylabel('Valor Real')
plt.title('Matriz de Confusión - MLP 2')
plt.show()

# Curva de Loss
plt.figure(figsize=(8, 5))
plt.plot(history_final.history['loss'], label='Train Loss')
plt.plot(history_final.history['val_loss'], label='Validation Loss')
plt.xlabel('Épocas')
plt.ylabel('Loss')
plt.title('Curva de Loss durante el Entrenamiento - MLP 2')
plt.legend()
plt.show()

"""#### **MLP 3**"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical

from sklearn.model_selection import KFold
from sklearn.metrics import (
    confusion_matrix,
    classification_report,
    accuracy_score,
    roc_auc_score,
    precision_score,
    recall_score,
    f1_score
)


if not isinstance(X_train, np.ndarray):
    X_train = X_train.values
if not isinstance(y_train, np.ndarray):
    y_train = y_train.values
if not isinstance(X_test, np.ndarray):
    X_test = X_test.values
if not isinstance(y_test, np.ndarray):
    y_test = y_test.values

# One‑hot para usar categorical_crossentropy (problema binario)
y_train_cat = to_categorical(y_train, num_classes=2)
y_test_cat  = to_categorical(y_test,  num_classes=2)

# Definir dimensiones
input_dim     = X_train.shape[1]
hidden1_mlp3  = max(1, int(0.5 * input_dim))       # 50% de input_dim
hidden2_mlp3  = max(1, int(0.5 * hidden1_mlp3))    # 50% de hidden1

def create_mlp3_model(input_dim):
    model = Sequential()
    model.add(Dense(hidden1_mlp3, input_dim=input_dim, activation='relu'))
    model.add(Dense(hidden2_mlp3, activation='relu'))
    model.add(Dense(2, activation='softmax'))  # 2 clases
    model.compile(
        optimizer=Adam(learning_rate=1e-4),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=20,
    restore_best_weights=True
)

# --- K-Fold Cross Validation (10 folds) ---
kf = KFold(n_splits=10, shuffle=True, random_state=42)
fold_results = []
fold_no = 1

for train_index, val_index in kf.split(X_train):
    X_fold_train = X_train[train_index]
    X_fold_val   = X_train[val_index]
    y_fold_train = y_train_cat[train_index]
    y_fold_val   = y_train_cat[val_index]

    model = create_mlp3_model(input_dim)
    history = model.fit(
        X_fold_train, y_fold_train,
        validation_data=(X_fold_val, y_fold_val),
        epochs=1000,
        batch_size=32,
        callbacks=[early_stopping],
        verbose=0
    )

    # Evaluación sobre el fold
    loss, keras_acc = model.evaluate(X_fold_val, y_fold_val, verbose=0)

    # Predicciones para métricas adicionales
    y_val_prob = model.predict(X_fold_val, verbose=0)
    y_val_pred = np.argmax(y_val_prob, axis=1)
    y_val_true = np.argmax(y_fold_val, axis=1)

    # Cálculo de métricas sklearn
    acc   = accuracy_score(y_val_true, y_val_pred)
    auc   = roc_auc_score(y_val_true, y_val_prob[:,1])
    prec  = precision_score(y_val_true, y_val_pred)
    rec   = recall_score(y_val_true, y_val_pred)
    f1    = f1_score(y_val_true, y_val_pred)

    fold_results.append({
        "Fold": fold_no,
        "Epochs": len(history.history['loss']),
        "Val_Loss": loss,
        "Keras_Acc": keras_acc,
        "Accuracy": acc,
        "AUC": auc,
        "Precision": prec,
        "Recall": rec,
        "F1": f1
    })

    print(f"Fold {fold_no} → Loss: {loss:.4f}, Acc: {acc:.4f}, "
          f"AUC: {auc:.4f}, Prec: {prec:.4f}, Rec: {rec:.4f}, F1: {f1:.4f}")
    fold_no += 1

# Crear DataFrame y mostrar resultados por fold
results_df = pd.DataFrame(fold_results)
print("\nResultados por fold:")
print(results_df.to_string(index=False))

# -----------------------------
# Resumen de métricas: promedio y desviación estándar
stats   = results_df[[
    "Val_Loss", "Keras_Acc",
    "Accuracy", "AUC",
    "Precision", "Recall", "F1"
]]
summary = stats.agg(["mean", "std"]).transpose().reset_index()
summary.columns = ["Métrica", "Promedio", "Desviación estándar"]

print("\nMétricas promedio y SD (10 folds):")
print(summary.to_markdown(index=False, floatfmt=".4f"))

# -------------------------------------------------------------------
# Entrenamiento final con todo el set de entrenamiento
final_model = create_mlp3_model(input_dim)
history_final = final_model.fit(
    X_train, y_train_cat,
    validation_split=0.2,
    epochs=1000,
    batch_size=32,
    callbacks=[early_stopping],
    verbose=1
)

# Evaluación en el set de prueba
loss_test, acc_test = final_model.evaluate(X_test, y_test_cat, verbose=0)
print(f"\nMLP 3 - Loss test: {loss_test:.4f}, Accuracy test: {acc_test:.4f}")

# Reporte de clasificación y matriz de confusión en test
y_pred_prob = final_model.predict(X_test, verbose=0)
y_pred      = np.argmax(y_pred_prob, axis=1)
y_true      = np.argmax(y_test_cat, axis=1)

print("\nReporte de Clasificación en test:")
print(classification_report(y_true, y_pred))

cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicción')
plt.ylabel('Valor Real')
plt.title('Matriz de Confusión - MLP 3')
plt.show()

# Curva de Loss durante el entrenamiento
plt.figure(figsize=(8, 5))
plt.plot(history_final.history['loss'], label='Train Loss')
plt.plot(history_final.history['val_loss'], label='Validation Loss')
plt.xlabel('Épocas')
plt.ylabel('Loss')
plt.title('Curva de Loss durante el Entrenamiento - MLP 3')
plt.legend()
plt.show()

"""### **SVM**

#### **SVM con Kernel Lineal**
"""

from sklearn.impute import SimpleImputer
from sklearn.svm import SVC
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    roc_auc_score,
    confusion_matrix,
    classification_report,
    f1_score
)
from sklearn.model_selection import StratifiedKFold
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

# Supongamos que X_train, y_train están definidos
X = X_train.copy()
y = y_train.copy()

# Listas para almacenar métricas
accuracy_list = []
precision_list = []
recall_list = []
auc_list = []
f1_list = []
fold_confusion_matrices = []

cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
fold_number = 1

for train_index, test_index in cv.split(X, y):
    # Partición de cada fold
    if isinstance(X, pd.DataFrame):
        X_tr, X_te = X.iloc[train_index], X.iloc[test_index]
    else:
        X_tr, X_te = X[train_index], X[test_index]
    if isinstance(y, (pd.Series, pd.DataFrame)):
        y_tr, y_te = y.iloc[train_index], y.iloc[test_index]
    else:
        y_tr, y_te = y[train_index], y[test_index]

    imputer = SimpleImputer(strategy='mean')
    X_tr_imputed = imputer.fit_transform(X_tr)
    X_te_imputed = imputer.transform(X_te)

    # Entrenar SVM con kernel lineal
    model = SVC(kernel='linear', probability=True, random_state=42)
    model.fit(X_tr_imputed, y_tr)

    # Predicciones
    y_pred = model.predict(X_te_imputed)
    y_proba = model.predict_proba(X_te_imputed)

    # Cálculo de métricas
    acc = accuracy_score(y_te, y_pred)
    if len(np.unique(y)) == 2:
        auc  = roc_auc_score(y_te, y_proba[:, 1])
        prec = precision_score(y_te, y_pred, average='binary')
        rec  = recall_score(y_te, y_pred, average='binary')
        f1   = f1_score(y_te, y_pred, average='binary')
    else:
        auc  = roc_auc_score(y_te, y_proba, multi_class='ovr')
        prec = precision_score(y_te, y_pred, average='macro')
        rec  = recall_score(y_te, y_pred, average='macro')
        f1   = f1_score(y_te, y_pred, average='macro')

    # Guardar resultados
    accuracy_list.append(acc)
    precision_list.append(prec)
    recall_list.append(rec)
    auc_list.append(auc)
    f1_list.append(f1)
    fold_confusion_matrices.append(confusion_matrix(y_te, y_pred))

    # Mostrar fold
    print(f"Fold {fold_number} - Exactitud: {acc:.4f}, Precisión: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}")
    print(classification_report(y_te, y_pred))
    print("-" * 50)
    fold_number += 1

# Cálculo de medias y SD de muestra (ddof=1)
mean_accuracy  = np.mean(accuracy_list)
std_accuracy   = np.std(accuracy_list, ddof=1)
mean_precision = np.mean(precision_list)
std_precision  = np.std(precision_list, ddof=1)
mean_recall    = np.mean(recall_list)
std_recall     = np.std(recall_list, ddof=1)
mean_f1        = np.mean(f1_list)
std_f1         = np.std(f1_list, ddof=1)
mean_auc       = np.mean(auc_list)
std_auc        = np.std(auc_list, ddof=1)

print("Resumen de validación cruzada (10 folds):")
print(f"Exactitud promedio: {mean_accuracy:.4f} ± {std_accuracy:.4f}")
print(f"Precisión promedio: {mean_precision:.4f} ± {std_precision:.4f}")
print(f"Recall promedio: {mean_recall:.4f} ± {std_recall:.4f}")
print(f"F1 promedio: {mean_f1:.4f} ± {std_f1:.4f}")
print(f"AUC promedio: {mean_auc:.4f} ± {std_auc:.4f}")

# Matriz de confusión global
global_cm = np.sum(fold_confusion_matrices, axis=0)
plt.figure(figsize=(6, 4))
sns.heatmap(global_cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicción")
plt.ylabel("Valor Real")
plt.title("Matriz de Confusión Global - SVM Kernel Lineal (CV)")
plt.show()

"""#### **SVM con Kernel Polinomial**"""

import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    roc_auc_score,
    confusion_matrix,
    classification_report,
    f1_score
)
from sklearn.model_selection import StratifiedKFold
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd


X = X_train.copy()
y = y_train.copy()

# Inicializar listas para almacenar las métricas de cada fold
accuracy_list = []
precision_list = []
recall_list = []
auc_list = []
f1_list = []
fold_confusion_matrices = []

# Configurar K-Fold Cross Validation con 10 splits (con StratifiedKFold para mantener la distribución de clases)
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

fold_number = 1
for train_index, test_index in cv.split(X, y):
    if isinstance(X, pd.DataFrame):
        X_train_fold = X.iloc[train_index]
        X_test_fold = X.iloc[test_index]
    else:
        X_train_fold = X[train_index]
        X_test_fold = X[test_index]

    if isinstance(y, (pd.Series, pd.DataFrame)):
        y_train_fold = y.iloc[train_index]
        y_test_fold = y.iloc[test_index]
    else:
        y_train_fold = y[train_index]
        y_test_fold = y[test_index]

    # Definir el modelo SVM con kernel lineal y probability=True (necesario para calcular el AUC)
    model = SVC(kernel='linear', random_state=42, probability=True)
    model.fit(X_train_fold, y_train_fold)

    # Realizar predicciones y obtener probabilidades
    y_pred_fold = model.predict(X_test_fold)
    y_proba_fold = model.predict_proba(X_test_fold)

    # Calcular métricas de desempeño
    acc = accuracy_score(y_test_fold, y_pred_fold)
    if len(np.unique(y)) == 2:  # problema binario
        auc = roc_auc_score(y_test_fold, y_proba_fold[:, 1])
        prec = precision_score(y_test_fold, y_pred_fold, average='binary')
        rec = recall_score(y_test_fold, y_pred_fold, average='binary')
        f1 = f1_score(y_test_fold, y_pred_fold, average='binary')
    else:
        auc = roc_auc_score(y_test_fold, y_proba_fold, multi_class='ovr')
        prec = precision_score(y_test_fold, y_pred_fold, average='macro')
        rec = recall_score(y_test_fold, y_pred_fold, average='macro')
        f1 = f1_score(y_test_fold, y_pred_fold, average='macro')

    accuracy_list.append(acc)
    precision_list.append(prec)
    recall_list.append(rec)
    auc_list.append(auc)
    f1_list.append(f1)

    cm = confusion_matrix(y_test_fold, y_pred_fold)
    fold_confusion_matrices.append(cm)

    print(f"Fold {fold_number} - Exactitud: {acc:.4f}, Precisión: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}")
    print(classification_report(y_test_fold, y_pred_fold))
    print("-" * 50)
    fold_number += 1

mean_accuracy = np.mean(accuracy_list)
mean_precision = np.mean(precision_list)
mean_recall = np.mean(recall_list)
mean_auc = np.mean(auc_list)
mean_f1 = np.mean(f1_list)

std_accuracy = np.std(accuracy_list)
std_precision = np.std(precision_list)
std_recall = np.std(recall_list)
std_auc = np.std(auc_list)
std_f1 = np.std(f1_list)

print("Resumen de validación cruzada (10 folds):")
print(f"Exactitud promedio: {mean_accuracy:.4f} ± {std_accuracy:.4f}")
print(f"Precisión promedio: {mean_precision:.4f} ± {std_precision:.4f}")
print(f"Recall promedio: {mean_recall:.4f} ± {std_recall:.4f}")
print(f"F1 promedio: {mean_f1:.4f} ± {std_f1:.4f}")
print(f"AUC promedio: {mean_auc:.4f} ± {std_auc:.4f}")

tabla_resumen = pd.DataFrame({
    "Métrica": ["Exactitud Promedio", "Precisión Promedio", "Recall Promedio", "F1 Promedio", "AUC Promedio"],
    "Promedio": [mean_accuracy, mean_precision, mean_recall, mean_f1, mean_auc],
    "±": [std_accuracy, std_precision, std_recall, std_f1, std_auc]
})

print("\nTabla resumen de métricas promedio (10-Fold CV):")
print(tabla_resumen)

global_cm = np.sum(fold_confusion_matrices, axis=0)
plt.figure(figsize=(6, 4))
sns.heatmap(global_cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicción")
plt.ylabel("Valor Real")
plt.title("Matriz de Confusión Global - SVM Kernel Lineal (CV)")
plt.show()

"""#### **SVM con Kernel Radial**"""

import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    roc_auc_score,
    confusion_matrix,
    classification_report,
    f1_score
)
from sklearn.model_selection import StratifiedKFold
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Inicializar listas para almacenar las métricas de cada fold
accuracy_list = []
precision_list = []
recall_list = []
auc_list = []
f1_list = []
fold_confusion_matrices = []

# Configurar la validación cruzada estratificada en 10 folds para mantener la distribución de clases
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

fold_number = 1
for train_index, test_index in cv.split(X, y):
    # Si X o y son pandas (DataFrame o Series), se usa .iloc para la indexación posicional.
    if isinstance(X, (pd.DataFrame, pd.Series)):
        X_train_fold = X.iloc[train_index]
        X_test_fold = X.iloc[test_index]
    else:
        X_train_fold = X[train_index]
        X_test_fold = X[test_index]

    if isinstance(y, (pd.DataFrame, pd.Series)):
        y_train_fold = y.iloc[train_index]
        y_test_fold = y.iloc[test_index]
    else:
        y_train_fold = y[train_index]
        y_test_fold = y[test_index]

    # Definir y entrenar el modelo SVM con kernel radial (RBF) y probability=True (requerido para calcular el AUC)
    model = SVC(kernel='rbf', random_state=42, probability=True)
    model.fit(X_train_fold, y_train_fold)

    # Realizar predicciones y obtener las probabilidades
    y_pred_fold = model.predict(X_test_fold)
    y_proba_fold = model.predict_proba(X_test_fold)

    # Calcular las métricas de desempeño
    acc = accuracy_score(y_test_fold, y_pred_fold)
    if len(np.unique(y)) == 2:
        auc = roc_auc_score(y_test_fold, y_proba_fold[:, 1])
        prec = precision_score(y_test_fold, y_pred_fold, average='binary')
        rec = recall_score(y_test_fold, y_pred_fold, average='binary')
        f1 = f1_score(y_test_fold, y_pred_fold, average='binary')
    else:
        auc = roc_auc_score(y_test_fold, y_proba_fold, multi_class='ovr')
        prec = precision_score(y_test_fold, y_pred_fold, average='macro')
        rec = recall_score(y_test_fold, y_pred_fold, average='macro')
        f1 = f1_score(y_test_fold, y_pred_fold, average='macro')

    accuracy_list.append(acc)
    precision_list.append(prec)
    recall_list.append(rec)
    auc_list.append(auc)
    f1_list.append(f1)

    # Calcular la matriz de confusión para el fold actual
    cm = confusion_matrix(y_test_fold, y_pred_fold)
    fold_confusion_matrices.append(cm)

    print(f"Fold {fold_number} - Exactitud: {acc:.4f}, Precisión: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}")
    print(classification_report(y_test_fold, y_pred_fold))
    print("-" * 50)
    fold_number += 1

# Calcular las métricas promedio y sus desviaciones estándar
mean_accuracy = np.mean(accuracy_list)
mean_precision = np.mean(precision_list)
mean_recall = np.mean(recall_list)
mean_auc = np.mean(auc_list)
mean_f1 = np.mean(f1_list)

std_accuracy = np.std(accuracy_list)
std_precision = np.std(precision_list)
std_recall = np.std(recall_list)
std_auc = np.std(auc_list)
std_f1 = np.std(f1_list)

print("Resumen de validación cruzada (10 folds):")
print(f"Exactitud promedio: {mean_accuracy:.4f} ± {std_accuracy:.4f}")
print(f"Precisión promedio: {mean_precision:.4f} ± {std_precision:.4f}")
print(f"Recall promedio: {mean_recall:.4f} ± {std_recall:.4f}")
print(f"F1 promedio: {mean_f1:.4f} ± {std_f1:.4f}")
print(f"AUC promedio: {mean_auc:.4f} ± {std_auc:.4f}")

# Crear una tabla resumen con las métricas promediadas y sus desviaciones estándar
tabla_resumen = pd.DataFrame({
    "Métrica": ["Exactitud Promedio", "Precisión Promedio", "Recall Promedio", "F1 Promedio", "AUC Promedio"],
    "Promedio": [mean_accuracy, mean_precision, mean_recall, mean_f1, mean_auc],
    "±": [std_accuracy, std_precision, std_recall, std_f1, std_auc]
})

print("\nTabla resumen de métricas promedio (10-Fold CV):")
print(tabla_resumen)

# Sumar las matrices de confusión de cada fold para obtener la matriz global
global_cm = np.sum(fold_confusion_matrices, axis=0)

# Visualizar la matriz de confusión global
plt.figure(figsize=(6, 4))
sns.heatmap(global_cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicción")
plt.ylabel("Valor Real")
plt.title("Matriz de Confusión Global - SVM Kernel RBF (CV)")
plt.show()

"""# **KNN**

#### **KNN (3 Neighbors)**
"""

import pandas as pd
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import StratifiedKFold, cross_validate, cross_val_predict, train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
import seaborn as sns

target = df['InjDefn_binary']

X_train, X_test, y_train, y_test = train_test_split(
    features, target, test_size=0.3, random_state=42
)

imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed  = imputer.transform(X_test)

knn_3 = KNeighborsClassifier(n_neighbors=3)
skf   = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

scoring = {
    'accuracy':  'accuracy',
    'precision': 'precision',
    'recall':    'recall',
    'roc_auc':   'roc_auc',
    'f1':        'f1'
}

cv_results = cross_validate(
    knn_3, X_train_imputed, y_train,
    cv=skf, scoring=scoring
)

folds_df = pd.DataFrame({
    "Fold":      np.arange(1, 11),
    "Accuracy":  cv_results['test_accuracy'],
    "Precision": cv_results['test_precision'],
    "Recall":    cv_results['test_recall'],
    "AUC":       cv_results['test_roc_auc'],
    "F1":        cv_results['test_f1']
})
print("Resultados de cada fold (KNN con 3 vecinos):")
print(folds_df)

# Medias
means = { m: np.mean(cv_results[f'test_{m}']) for m in scoring }
# SD muestral
stds  = { m: np.std(cv_results[f'test_{m}'], ddof=1) for m in scoring }

avg_metrics_df = pd.DataFrame({
    "Métrica":  ["Accuracy", "Precision", "Recall", "AUC", "F1"],
    "Promedio": [means['accuracy'], means['precision'], means['recall'], means['roc_auc'], means['f1']],
    "SD":       [stds['accuracy'],  stds['precision'],  stds['recall'],  stds['roc_auc'],  stds['f1']]
})

print("\nTabla de métricas promedio (± SD muestral):")
with pd.option_context('display.float_format', '{:0.4f}'.format):
    print(avg_metrics_df)

# Reporte y matriz de confusión
y_pred_cv = cross_val_predict(knn_3, X_train_imputed, y_train, cv=skf)

print("\nReporte de Clasificación (CV):")
print(classification_report(y_train, y_pred_cv))

cm = confusion_matrix(y_train, y_pred_cv)
print("Matriz de Confusión:")
print(cm)

plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Matriz de Confusión – KNN (3 vecinos)")
plt.xlabel("Predicción")
plt.ylabel("Real")
plt.show()

"""#### **KNN (5 Neighbors)**"""

import pandas as pd
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import StratifiedKFold, cross_validate, cross_val_predict, train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
import seaborn as sns

target = df['InjDefn_binary']

# División entrenamiento/prueba
X_train, X_test, y_train, y_test = train_test_split(
    features, target, test_size=0.3, random_state=42
)

# Imputación de valores faltantes
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed  = imputer.transform(X_test)

# Clasificador KNN con 5 vecinos
knn_5 = KNeighborsClassifier(n_neighbors=5)
skf   = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Métricas a evaluar
scoring = {
    'accuracy':  'accuracy',
    'precision': 'precision',
    'recall':    'recall',
    'roc_auc':   'roc_auc',
    'f1':        'f1'
}

# Validación cruzada
cv_results = cross_validate(
    knn_5, X_train_imputed, y_train,
    cv=skf, scoring=scoring
)

# Resultados por fold
folds_df = pd.DataFrame({
    "Fold":      np.arange(1, 11),
    "Accuracy":  cv_results['test_accuracy'],
    "Precision": cv_results['test_precision'],
    "Recall":    cv_results['test_recall'],
    "AUC":       cv_results['test_roc_auc'],
    "F1":        cv_results['test_f1']
})
print("Resultados de cada fold (KNN con 5 vecinos):")
print(folds_df)

# Cálculo de medias y SD muestral
means = { m: np.mean(cv_results[f'test_{m}']) for m in scoring }
stds  = { m: np.std(cv_results[f'test_{m}'], ddof=1) for m in scoring }

avg_metrics_df = pd.DataFrame({
    "Métrica":  ["Accuracy", "Precision", "Recall", "AUC", "F1"],
    "Promedio": [means['accuracy'], means['precision'], means['recall'], means['roc_auc'], means['f1']],
    "SD":       [stds['accuracy'],  stds['precision'],  stds['recall'],  stds['roc_auc'],  stds['f1']]
})

print("\nTabla de métricas promedio (± SD muestral):")
with pd.option_context('display.float_format', '{:0.4f}'.format):
    print(avg_metrics_df)

# Reporte de clasificación y matriz de confusión usando CV
y_pred_cv = cross_val_predict(knn_5, X_train_imputed, y_train, cv=skf)

print("\nReporte de Clasificación (CV):")
print(classification_report(y_train, y_pred_cv))

cm = confusion_matrix(y_train, y_pred_cv)
print("Matriz de Confusión:")
print(cm)

# Visualización de la matriz de confusión
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Matriz de Confusión – KNN (5 vecinos)")
plt.xlabel("Predicción")
plt.ylabel("Real")
plt.show()

"""#### **KNN (7 Neighbors)**"""

import pandas as pd
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import StratifiedKFold, cross_validate, cross_val_predict, train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
import seaborn as sns


target = df['InjDefn_binary']

# Dividir el dataset en entrenamiento (70%) y prueba (30%)
X_train, X_test, y_train, y_test = train_test_split(
    features, target, test_size=0.3, random_state=42
)

# Imputar valores perdidos (NaN)
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed  = imputer.transform(X_test)

# Modelo KNN con 7 vecinos
knn_7 = KNeighborsClassifier(n_neighbors=7)

# CV estratificada a 10 folds
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Métricas a evaluar
scoring = {
    'accuracy':  'accuracy',
    'precision': 'precision',
    'recall':    'recall',
    'roc_auc':   'roc_auc',
    'f1':        'f1'
}

# Ejecutar CV
cv_results = cross_validate(
    knn_7,
    X_train_imputed,
    y_train,
    cv=skf,
    scoring=scoring
)

# Resultados por fold
folds_df = pd.DataFrame({
    "Fold":      np.arange(1, len(cv_results['test_accuracy']) + 1),
    "Accuracy":  cv_results['test_accuracy'],
    "Precision": cv_results['test_precision'],
    "Recall":    cv_results['test_recall'],
    "AUC":       cv_results['test_roc_auc'],
    "F1":        cv_results['test_f1']
})
print("Resultados de cada fold (KNN con 7 vecinos):")
print(folds_df)

# Medias
acc_mean  = np.mean(cv_results['test_accuracy'])
auc_mean  = np.mean(cv_results['test_roc_auc'])
prec_mean = np.mean(cv_results['test_precision'])
rec_mean  = np.mean(cv_results['test_recall'])
f1_mean   = np.mean(cv_results['test_f1'])

# Desviaciones estándar muestrales (ddof=1)
acc_std  = np.std(cv_results['test_accuracy'],  ddof=1)
auc_std  = np.std(cv_results['test_roc_auc'],   ddof=1)
prec_std = np.std(cv_results['test_precision'], ddof=1)
rec_std  = np.std(cv_results['test_recall'],    ddof=1)
f1_std   = np.std(cv_results['test_f1'],        ddof=1)

# Impresión individual
print("\nMétricas promedio (± SD muestral):")
print(f"Accuracy promedio:  {acc_mean:.4f} ± {acc_std:.4f}")
print(f"AUC promedio:       {auc_mean:.4f} ± {auc_std:.4f}")
print(f"Precisión promedio: {prec_mean:.4f} ± {prec_std:.4f}")
print(f"Recall promedio:    {rec_mean:.4f} ± {rec_std:.4f}")
print(f"F1 promedio:        {f1_mean:.4f} ± {f1_std:.4f}")

# Tabla resumen
avg_metrics_df = pd.DataFrame({
    "Métrica":  ["Accuracy", "AUC", "Precision", "Recall", "F1"],
    "Promedio": [acc_mean, auc_mean, prec_mean, rec_mean, f1_mean],
    "SD":       [acc_std, auc_std, prec_std, rec_std, f1_std]
})
print("\nTabla de métricas promedio (± SD muestral):")
with pd.option_context('display.float_format', '{:0.4f}'.format):
    print(avg_metrics_df)

# Reporte y matriz de confusión vía CV-predict
y_pred_cv = cross_val_predict(knn_7, X_train_imputed, y_train, cv=skf)

print("\nReporte de Clasificación (CV):")
print(classification_report(y_train, y_pred_cv))

cm = confusion_matrix(y_train, y_pred_cv)
print("Matriz de Confusión:")
print(cm)

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicción")
plt.ylabel("Valor Real")
plt.title("Matriz de Confusión - 10-Fold CV (KNN con 7 vecinos)")
plt.show()

"""# **Naive Bayes**"""

import pandas as pd
import numpy as np
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import StratifiedKFold, cross_validate, cross_val_predict, train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
import seaborn as sns


target = df['InjDefn_binary']

# Dividir el dataset en entrenamiento (70%) y prueba (30%)
X_train, X_test, y_train, y_test = train_test_split(
    features, target, test_size=0.3, random_state=42
)

# Imputar valores perdidos (NaN) en X_train y X_test
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed  = imputer.transform(X_test)

# Definir el modelo Naive Bayes (GaussianNB)
nb_model = GaussianNB()

# Validación cruzada estratificada con 10 folds
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Métricas a evaluar
scoring = {
    'accuracy':  'accuracy',
    'precision': 'precision',
    'recall':    'recall',
    'roc_auc':   'roc_auc',
    'f1':        'f1'
}

# Ejecutar CV y capturar resultados
cv_results = cross_validate(
    nb_model,
    X_train_imputed,
    y_train,
    cv=skf,
    scoring=scoring
)

# Resultados por cada fold
folds_df = pd.DataFrame({
    "Fold":      np.arange(1, len(cv_results['test_accuracy']) + 1),
    "Accuracy":  cv_results['test_accuracy'],
    "Precision": cv_results['test_precision'],
    "Recall":    cv_results['test_recall'],
    "AUC":       cv_results['test_roc_auc'],
    "F1":        cv_results['test_f1']
})
print("Resultados de cada fold (Naive Bayes):")
print(folds_df)

# Cálculo de medias
mean_accuracy  = np.mean(cv_results['test_accuracy'])
mean_precision = np.mean(cv_results['test_precision'])
mean_recall    = np.mean(cv_results['test_recall'])
mean_auc       = np.mean(cv_results['test_roc_auc'])
mean_f1        = np.mean(cv_results['test_f1'])

# Cálculo de desviaciones estándar muestrales (ddof=1)
std_accuracy  = np.std(cv_results['test_accuracy'],  ddof=1)
std_precision = np.std(cv_results['test_precision'], ddof=1)
std_recall    = np.std(cv_results['test_recall'],    ddof=1)
std_auc       = np.std(cv_results['test_roc_auc'],   ddof=1)
std_f1        = np.std(cv_results['test_f1'],        ddof=1)

# Impresión individual de métricas promedio ± SD
print("\nMétricas promedio (± SD muestral):")
print(f"Accuracy promedio:  {mean_accuracy:.4f} ± {std_accuracy:.4f}")
print(f"Precisión promedio: {mean_precision:.4f} ± {std_precision:.4f}")
print(f"Recall promedio:    {mean_recall:.4f} ± {std_recall:.4f}")
print(f"AUC promedio:       {mean_auc:.4f} ± {std_auc:.4f}")
print(f"F1 promedio:        {mean_f1:.4f} ± {std_f1:.4f}")

# Tabla resumen de métricas promedio con SD
avg_metrics_df = pd.DataFrame({
    "Métrica":  ["Accuracy", "Precision", "Recall", "AUC", "F1"],
    "Promedio": [mean_accuracy, mean_precision, mean_recall, mean_auc, mean_f1],
    "SD":       [std_accuracy,  std_precision,  std_recall,  std_auc,  std_f1]
})
print("\nTabla de métricas promedio (± SD muestral):")
with pd.option_context('display.float_format', '{:0.4f}'.format):
    print(avg_metrics_df)

# Reporte de clasificación y matriz de confusión usando cross_val_predict
y_pred_cv = cross_val_predict(nb_model, X_train_imputed, y_train, cv=skf)

print("\nReporte de Clasificación (CV):")
print(classification_report(y_train, y_pred_cv))

cm = confusion_matrix(y_train, y_pred_cv)
print("Matriz de Confusión:")
print(cm)

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicción")
plt.ylabel("Valor Real")
plt.title("Matriz de Confusión – CV (10 folds) para Naive Bayes")
plt.show()